{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dwdb/dependency-parser/blob/master/fast_dpnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "jYT5zZuMV9O8",
    "outputId": "b9264177-df1b-414a-82fb-da121e4cd71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 15 11:20:13 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:35:00.0 Off |                  N/A |\n",
      "| 30%   39C    P2   113W / 350W |   3024MiB / 24576MiB |     18%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:36:00.0 Off |                  N/A |\n",
      "| 33%   52C    P2   181W / 350W |  13813MiB / 24576MiB |     74%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:39:00.0 Off |                  N/A |\n",
      "| 30%   43C    P2   110W / 350W |   4173MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 30%   40C    P2   102W / 350W |  16875MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:9C:00.0 Off |                  N/A |\n",
      "| 31%   47C    P2   250W / 350W |   5415MiB / 24576MiB |     36%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:9D:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8    18W / 350W |     15MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce ...  Off  | 00000000:A0:00.0 Off |                  N/A |\n",
      "| 30%   29C    P8    13W / 350W |     17MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce ...  Off  | 00000000:A4:00.0 Off |                  N/A |\n",
      "| 30%   41C    P2   127W / 350W |   8725MiB / 24576MiB |     22%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    253277      C   ...nda3/envs/quzm/bin/python      435MiB |\n",
      "|    0   N/A  N/A   1259113      C   python                            673MiB |\n",
      "|    0   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   3021985      C   ...nda3/envs/quzm/bin/python      435MiB |\n",
      "|    1   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2925099      C   python                           6901MiB |\n",
      "|    1   N/A  N/A   2928224      C   python                           6899MiB |\n",
      "|    2   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A   2498622      C   python3                          3787MiB |\n",
      "|    2   N/A  N/A   3348595      C   python3                           367MiB |\n",
      "|    3   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A   2499530      C   python3                          1733MiB |\n",
      "|    3   N/A  N/A   3021985      C   ...nda3/envs/quzm/bin/python    15129MiB |\n",
      "|    4   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    4   N/A  N/A    253277      C   ...nda3/envs/quzm/bin/python     4989MiB |\n",
      "|    4   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    4   N/A  N/A   3021985      C   ...nda3/envs/quzm/bin/python      413MiB |\n",
      "|    5   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    6   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    6   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    7   N/A  N/A      2770      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    7   N/A  N/A   2399011      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    7   N/A  N/A   2893954      C   ...nvs/fyj_models/bin/python     1419MiB |\n",
      "|    7   N/A  N/A   2942212      C   python                           7287MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: /device:CPU:0, Type: CPU\n",
      "Device: /device:XLA_CPU:0, Type: XLA_CPU\n",
      "Device: /device:XLA_GPU:0, Type: XLA_GPU\n",
      "Device: /device:GPU:0, Type: GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # 只设置 GPU 5 为可见设备（忽略其他 GPU）\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        \n",
    "        # 如果需要按需分配内存（可选）\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def print_device_info():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        print(f\"Device: {device.name}, Type: {device.device_type}\")\n",
    "\n",
    "print_device_info()  # 在程序开始时调用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaJUfn_MWHuq"
   },
   "source": [
    "# 依存句法单词节点类\n",
    "节点id、词性、头节点id、左右孩子节点（指向的节点）、左右依存关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC0n1fvYV-uf"
   },
   "outputs": [],
   "source": [
    "class Token(object):\n",
    "\n",
    "    def __init__(self, token_id, word, pos, dep, head_id):\n",
    "        self.token_id = token_id\n",
    "        self.word = word.lower()\n",
    "        self.pos = pos\n",
    "        if head_id >= token_id:\n",
    "            self.dep = 'L_' + dep\n",
    "        else:\n",
    "            self.dep = 'R_' + dep\n",
    "        self.head_id = head_id\n",
    "        self.left = []\n",
    "        self.right = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Token(token_id={token_id},word={word},head_id={head_id})'.format(\n",
    "            **self.__dict__)\n",
    "\n",
    "\n",
    "ROOT_TOKEN = Token(-1, '<root>', '<root>', '<root>', -1)\n",
    "NULL_TOKEN = Token(-1, '<null>', '<null>', '<null>', -1)\n",
    "UNK_TOKEN = Token(-1, '<unk>', '<unk>', '<unk>', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxK-Z8YGWtff"
   },
   "source": [
    "# 依存句法transfer-reduce句子类\n",
    "节点、缓冲、栈，通过get_next_input可获取当前状态下的单词、磁性、依存关系共计18+18+12个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9Tbr-9PWGZe"
   },
   "outputs": [],
   "source": [
    "class Sentence(object):\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.buff = tokens.copy()\n",
    "        self.stack = [ROOT_TOKEN]\n",
    "        self.deps = []\n",
    "\n",
    "    def update_by_action(self, action=None):\n",
    "        if action is None:\n",
    "            action = self.get_action()\n",
    "        # 转移\n",
    "        if action == 'shift':\n",
    "            self.stack.append(self.buff.pop(0))\n",
    "        # 左弧 stack1 <- stack0\n",
    "        elif action.startswith('L_'):\n",
    "            token = self.stack.pop(-2)\n",
    "            token.dep = action\n",
    "            self.deps.append((self.stack[-1].token_id, token.token_id, action))\n",
    "            self.binary_insert(self.stack[-1].left, token)\n",
    "        # 右弧 stack1 -> stack0\n",
    "        elif action.startswith('R_'):\n",
    "            token = self.stack.pop(-1)\n",
    "            token.dep = action\n",
    "            self.deps.append((self.stack[-1].token_id, token.token_id, action))\n",
    "            self.binary_insert(self.stack[-1].right, token)\n",
    "        else:\n",
    "            raise ValueError('unknown state!')\n",
    "        return action\n",
    "\n",
    "    def get_action(self):\n",
    "        if len(self.stack) < 2:\n",
    "            return 'shift'\n",
    "        t1, t0 = self.stack[-2:]\n",
    "        # left arc\n",
    "        if t1.head_id == t0.token_id:\n",
    "            return t1.dep\n",
    "        # right arc\n",
    "        if t0.head_id == t1.token_id:\n",
    "            if any(t0.token_id == t.head_id for t in self.buff):\n",
    "                return 'shift'\n",
    "            return t0.dep\n",
    "        return 'shift'\n",
    "\n",
    "    def get_next_input(self, word2id, pos2id, dep2id):\n",
    "        \"\"\"将conll dataset转为fast dependency parser dataset\n",
    "        18 features of words and pos tags:\n",
    "            The top 3 words on the stack and buffer:\n",
    "                s1, s2, s3, b1, b2, b3\n",
    "            The first and second leftmost/rightmost children of top 2 words on stack:\n",
    "                lc1(s1), rc1(s1), lc2(s1), rc2(s1), lc1(s2), rc1(s2), lc2(s2), rc2(s2)\n",
    "            The leftmost/rightmost of leftmost/rightmost of top 2 words on stack:\n",
    "                lc1(lc1(s1)), rc1(rc1(s1)), lc1(lc1(s2)), rc1(rc1(s2))\n",
    "        12 features of dependencies, that excluding those 6 (18-6=12) words on the stack/buffer\n",
    "        \"\"\"\n",
    "\n",
    "        def pad_tokens(tokens, maxlen):\n",
    "            tokens = tokens[:maxlen]\n",
    "            if len(tokens) < maxlen:\n",
    "                tokens += [NULL_TOKEN] * (maxlen - len(tokens))\n",
    "            return tokens\n",
    "\n",
    "        def get_children(token):\n",
    "            lc1, lc2 = pad_tokens(token.left, 2)\n",
    "            rc1, rc2 = pad_tokens(token.right, 2)\n",
    "            llc1, = pad_tokens(lc1.left, 1)\n",
    "            rrc1, = pad_tokens(rc1.right, 1)\n",
    "            return [lc1, rc1, lc2, rc2, llc1, rrc1]\n",
    "\n",
    "        # top 3 features on stack\n",
    "        s1, s2, s3 = pad_tokens(self.stack[-1::-1], 3)\n",
    "        # 18 features\n",
    "        tokens = [s1, s2, s3] + pad_tokens(self.buff, 3) + get_children(s1) + get_children(s2)\n",
    "        # word, pos tag and dependency indices\n",
    "        input_word = [word2id.get(token.word, word2id[UNK_TOKEN.word]) for token in tokens]\n",
    "        input_pos = [pos2id.get(token.pos, pos2id[UNK_TOKEN.pos]) for token in tokens]\n",
    "        input_dep = [dep2id.get(token.dep, dep2id[UNK_TOKEN.dep]) for token in tokens[6:]]\n",
    "        return input_word, input_pos, input_dep\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_insert(array, value, key=lambda x: x.token_id):\n",
    "        start, end = 0, len(array) - 1\n",
    "        while start <= end:\n",
    "            mid = int((start + end) / 2)\n",
    "            if key(value) >= key(array[mid]):\n",
    "                start = mid + 1\n",
    "            else:\n",
    "                end = mid - 1\n",
    "        array.insert(start, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axuRWJxnXSHS"
   },
   "source": [
    "# Conll数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SomeK2tBXScv",
    "outputId": "76dfca31-4dbf-4f81-f923-4c05b3189b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming at line 0\n",
      "transforming at line 5000\n",
      "train-CTB.conll: error count:3220, total count:8301, total examples:493483\n",
      "transforming at line 0\n",
      "dev-CTB.conll: error count:190, total count:534, total examples:30301\n"
     ]
    }
   ],
   "source": [
    "class ConllDataset(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, encoding='utf8') as f:\n",
    "            dataset, tokens = [], []\n",
    "            for line in f.readlines():\n",
    "                if line == '\\n':\n",
    "                    dataset.append(Sentence(tokens))\n",
    "                    tokens = []\n",
    "                else:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    token = Token(int(line[0]) - 1, line[1], line[4], line[7], int(line[6]) - 1)\n",
    "                    tokens.append(token)\n",
    "            if tokens:\n",
    "                dataset.append(Sentence(tokens))\n",
    "        return dataset\n",
    "\n",
    "    def fit_transform(self, path, min_count=2, shuffle=True):\n",
    "        dataset = self.load(path)\n",
    "        # build vocabulary\n",
    "        vocab = defaultdict(int)\n",
    "        pos_tags, deps = set(), set()\n",
    "        for sentence in dataset:\n",
    "            for token in sentence.tokens:\n",
    "                vocab[token.word] += 1\n",
    "                pos_tags.add(token.pos)\n",
    "                deps.add(token.dep)\n",
    "\n",
    "        # create word dictionary\n",
    "        vocab = {k for k, v in vocab.items() if v >= min_count}\n",
    "        vocab.update((ROOT_TOKEN.word, NULL_TOKEN.word, UNK_TOKEN.word))\n",
    "        self.word2id = dict(zip(sorted(vocab), range(len(vocab))))\n",
    "        # create part-of-speech dictionary\n",
    "        pos_tags.update((ROOT_TOKEN.pos, NULL_TOKEN.pos, UNK_TOKEN.pos))\n",
    "        self.pos2id = dict(zip(sorted(pos_tags), range(len(pos_tags))))\n",
    "        # create dependency and labelsdictionary\n",
    "        deps.update((ROOT_TOKEN.dep, NULL_TOKEN.dep, UNK_TOKEN.dep))\n",
    "        labels = deps.copy() | {'shift', UNK_TOKEN.dep}\n",
    "        self.dep2id = dict(zip(sorted(deps), range(len(deps))))\n",
    "        self.id2label = sorted(labels)\n",
    "        self.label2id = dict(zip(self.id2label, range(len(self.id2label))))\n",
    "\n",
    "        self._fit = True\n",
    "        dataset = self.transform(path, dataset=dataset, shuffle=shuffle)\n",
    "        return dataset\n",
    "\n",
    "    def transform(self, path=None, dataset=None, shuffle=False):\n",
    "        if not dataset and path:\n",
    "            dataset = self.load(path)\n",
    "        assert getattr(self, '_fit', None), 'Model must be fit before transform!'\n",
    "\n",
    "        error_count = 0\n",
    "        inputs = []\n",
    "        for i, sentence in enumerate(dataset):\n",
    "            if i % 5000 == 0:\n",
    "                print('transforming at line %d' % i)\n",
    "            while len(sentence.stack) > 1 or sentence.buff:\n",
    "                input_word, input_pos, input_dep = sentence.get_next_input(\n",
    "                    self.word2id, self.pos2id, self.dep2id)\n",
    "                try:\n",
    "                    output = sentence.update_by_action()\n",
    "                    output = self.label2id.get(output, self.label2id[UNK_TOKEN.dep])\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    error_count += 1\n",
    "                    break\n",
    "                inputs.append((input_word, input_pos, input_dep, output))\n",
    "\n",
    "        # shuffle dataset\n",
    "        if shuffle:\n",
    "            np.random.shuffle(inputs)\n",
    "\n",
    "        print('%s: error count:%d, total count:%d, total examples:%d' % (\n",
    "            os.path.basename(str(path)), error_count, len(dataset), len(inputs)))\n",
    "        # input_word, input_pos, input_dep, output\n",
    "        return tuple(np.array(data, np.int32) for data in zip(*inputs))\n",
    "\n",
    "\n",
    "conll_path = '/data/fyj/project/dependency-parser/data/conll'\n",
    "conll = ConllDataset()\n",
    "train_dataset = conll.fit_transform(os.path.join(conll_path, 'train-CTB.conll'))\n",
    "valid_dataset = conll.transform(os.path.join(conll_path, 'dev-CTB.conll'))\n",
    "\n",
    "output_path = './output/CTB/'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "pickle.dump(conll.word2id, open(os.path.join(output_path, 'word2id.pkl'), 'wb'))\n",
    "pickle.dump(conll.pos2id, open(os.path.join(output_path, 'pos2id.pkl'), 'wb'))\n",
    "pickle.dump(conll.dep2id, open(os.path.join(output_path, 'dep2id.pkl'), 'wb'))\n",
    "pickle.dump(conll.label2id, open(os.path.join(output_path, 'label2id.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpTRzReiY9zs"
   },
   "source": [
    "# 创建fast dependency model\n",
    "单词、词性、依存关系存于同一字典，只需创建一个embedding变量即可，但是这样做不好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "NR-GAo9tYnhF",
    "outputId": "c7c3f03b-b23c-4ae6-e593-4d4bccaa0dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:12885, pos_size:40, dep_size:263, n_classes:264\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 18, 50)       644250      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 18, 50)       2000        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 12, 50)       13150       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 48, 50)]     0           embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 2400)]       0           tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2400)         0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          240100      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Pow_1 (TensorFlowOp [(None, 100)]        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           tf_op_layer_Pow_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 264)          26664       dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 926,164\n",
      "Trainable params: 926,164\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, pos_size, dep_size, embedding_size, n_classes):\n",
    "    print('vocab_size:%d, pos_size:%d, dep_size:%d, n_classes:%d' % (\n",
    "        vocab_size, pos_size, dep_size, n_classes))\n",
    "    l2_regularizer = tf.keras.regularizers.l2(1e-5)\n",
    "    # input layer\n",
    "    input_word = tf.keras.layers.Input(shape=(18,))\n",
    "    input_pos = tf.keras.layers.Input(shape=(18,))\n",
    "    input_dep = tf.keras.layers.Input(shape=(12,))\n",
    "    # embedding layer，initial weight range of [-0.01, 0.01]\n",
    "    word_embedding = tf.keras.layers.Embedding(\n",
    "        vocab_size, embedding_size,\n",
    "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
    "        embeddings_regularizer=l2_regularizer)(input_word)\n",
    "    pos_embedding = tf.keras.layers.Embedding(\n",
    "        pos_size, embedding_size,\n",
    "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
    "        embeddings_regularizer=l2_regularizer)(input_pos)\n",
    "    dep_embedding = tf.keras.layers.Embedding(\n",
    "        dep_size, embedding_size,\n",
    "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
    "        embeddings_regularizer=l2_regularizer)(input_dep)\n",
    "    # shape=(batch_size, 48, embedding_size)\n",
    "    embedding = tf.concat((word_embedding, pos_embedding, dep_embedding), axis=1)\n",
    "    embedding = tf.reshape(embedding, shape=(-1, embedding_size * 48))\n",
    "    embedding = tf.keras.layers.Dropout(rate=0.4)(embedding)\n",
    "    # dense layer\n",
    "    dense1 = tf.keras.layers.Dense(\n",
    "        units=100,\n",
    "        kernel_regularizer=l2_regularizer,\n",
    "        bias_regularizer=l2_regularizer)(embedding)\n",
    "    dense1 = tf.pow(dense1, 3)\n",
    "    dense1 = tf.keras.layers.Dropout(rate=0.4)(dense1)\n",
    "    # dense layer\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=n_classes,\n",
    "        kernel_regularizer=l2_regularizer,\n",
    "        bias_regularizer=l2_regularizer)(dense1)\n",
    "    model = tf.keras.Model(inputs=(input_word, input_pos, input_dep), outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocab_size=len(conll.word2id), pos_size=len(conll.pos2id), \n",
    "                    dep_size=len(conll.dep2id), embedding_size=50, n_classes=len(conll.label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "G0SXPJgRtaxZ",
    "outputId": "1ca279cd-7780-49a2-b9de-abdf2ff050c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "482/482 [==============================] - 7s 14ms/step - loss: 1.6633 - accuracy: 0.6738 - val_loss: 0.8113 - val_accuracy: 0.7908\n",
      "Epoch 2/10\n",
      "482/482 [==============================] - 6s 13ms/step - loss: 0.8326 - accuracy: 0.7858 - val_loss: 0.6748 - val_accuracy: 0.8223\n",
      "Epoch 3/10\n",
      "482/482 [==============================] - 6s 12ms/step - loss: 0.7295 - accuracy: 0.8099 - val_loss: 0.6282 - val_accuracy: 0.8331\n",
      "Epoch 4/10\n",
      "482/482 [==============================] - 6s 12ms/step - loss: 0.6800 - accuracy: 0.8211 - val_loss: 0.6086 - val_accuracy: 0.8371\n",
      "Epoch 5/10\n",
      "482/482 [==============================] - 6s 11ms/step - loss: 0.6502 - accuracy: 0.8288 - val_loss: 0.5958 - val_accuracy: 0.8408\n",
      "Epoch 6/10\n",
      "482/482 [==============================] - 5s 11ms/step - loss: 0.6301 - accuracy: 0.8331 - val_loss: 0.5920 - val_accuracy: 0.8444\n",
      "Epoch 7/10\n",
      "482/482 [==============================] - 6s 12ms/step - loss: 0.6142 - accuracy: 0.8375 - val_loss: 0.5862 - val_accuracy: 0.8464\n",
      "Epoch 8/10\n",
      "482/482 [==============================] - 6s 12ms/step - loss: 0.6025 - accuracy: 0.8403 - val_loss: 0.5883 - val_accuracy: 0.8469\n",
      "Epoch 9/10\n",
      "482/482 [==============================] - 6s 12ms/step - loss: 0.5936 - accuracy: 0.8438 - val_loss: 0.5841 - val_accuracy: 0.8486\n",
      "Epoch 10/10\n",
      "482/482 [==============================] - 5s 11ms/step - loss: 0.5864 - accuracy: 0.8454 - val_loss: 0.5875 - val_accuracy: 0.8481\n",
      "INFO:tensorflow:Assets written to: ./output/CTB/checkpoint/assets\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset[:3], train_dataset[3],\n",
    "    batch_size=1024, epochs=10, validation_data=(valid_dataset[:3], valid_dataset[3]))\n",
    "model.save(os.path.join(output_path, 'checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I95_moDHmggh",
    "outputId": "bd885c9a-6a99-4f64-fe2d-d7ef5c38e09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sentence:10, UAS:0.713, LAS:0.646\n",
      "total sentence:20, UAS:0.710, LAS:0.613\n",
      "total sentence:30, UAS:0.759, LAS:0.656\n",
      "total sentence:40, UAS:0.734, LAS:0.623\n",
      "total sentence:50, UAS:0.736, LAS:0.626\n",
      "total sentence:60, UAS:0.751, LAS:0.644\n",
      "total sentence:70, UAS:0.754, LAS:0.648\n",
      "total sentence:80, UAS:0.765, LAS:0.653\n",
      "total sentence:90, UAS:0.759, LAS:0.649\n",
      "total sentence:100, UAS:0.761, LAS:0.640\n",
      "total sentence:110, UAS:0.766, LAS:0.645\n",
      "total sentence:120, UAS:0.763, LAS:0.646\n",
      "total sentence:130, UAS:0.766, LAS:0.649\n",
      "total sentence:140, UAS:0.764, LAS:0.646\n",
      "total sentence:190, UAS:0.758, LAS:0.637\n",
      "total sentence:200, UAS:0.756, LAS:0.634\n",
      "total sentence:210, UAS:0.756, LAS:0.631\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "final_res=[]\n",
    "\n",
    "def evaluate(sentences, output_path):\n",
    "    # restore model\n",
    "    model = tf.keras.models.load_model(os.path.join(output_path, 'checkpoint'))\n",
    "    # load local dictionary\n",
    "    label2id = pickle.load(open(os.path.join(output_path, 'label2id.pkl'), 'rb'))\n",
    "    word2id = pickle.load(open(os.path.join(output_path, 'word2id.pkl'), 'rb'))\n",
    "    pos2id = pickle.load(open(os.path.join(output_path, 'pos2id.pkl'), 'rb'))\n",
    "    dep2id = pickle.load(open(os.path.join(output_path, 'dep2id.pkl'), 'rb'))\n",
    "    id2label, _ = zip(*sorted(label2id.items(), key=lambda x:x[1]))\n",
    "    \n",
    "    uas = las = count = 0\n",
    "    time_record = time.time()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        raw_deps = {}\n",
    "        for token in sentence.tokens:\n",
    "            raw_deps[(token.head_id, token.token_id)] = token.dep\n",
    "            token.dep = NULL_TOKEN.dep\n",
    "            token.head_id = -1\n",
    "        count += len(sentence.tokens)\n",
    "        while len(sentence.stack) > 1 or sentence.buff:\n",
    "            input_word, input_pos, input_dep = sentence.get_next_input(word2id, pos2id, dep2id)\n",
    "            input_word = np.array([input_word], dtype=np.int32)\n",
    "            input_pos = np.array([input_pos], dtype=np.int32)\n",
    "            input_dep = np.array([input_dep], dtype=np.int32)\n",
    "            output = model.predict((input_word, input_pos, input_dep))\n",
    "            action = id2label[np.argmax(output[0])]\n",
    "            try:\n",
    "                sentence.update_by_action(action)\n",
    "            except (IndexError, ValueError):\n",
    "                # print(len(sentence.tokens), len(sentence.deps))\n",
    "                break\n",
    "\n",
    "        for head, tail, action in sentence.deps:\n",
    "            raw_action = raw_deps.get((head, tail))\n",
    "            if raw_action is not None:\n",
    "                uas += 1\n",
    "                if raw_action == action:\n",
    "                    las += 1\n",
    "        if (i + 1) % 10 == 0 or i==len(sentences):\n",
    "            print('total sentence:%d, UAS:%.3f, LAS:%.3f' % (i + 1, uas / count, las / count))\n",
    "            res={}\n",
    "            res['uas']=uas / count\n",
    "            res['las']=las / count\n",
    "            res['second']=time.time()-time_record\n",
    "            final_res.append(res)\n",
    "\n",
    "sentences = ConllDataset.load(os.path.join(conll_path, 'test-CTB.conll'))\n",
    "evaluate(sentences, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPc7xPWbKe6JMtLlTIDT0Wd",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1e_GKf3_9rUaRtgSPBknQYQ57h-W2dWZM",
   "name": "fast_dpnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "fyj_dp",
   "language": "python",
   "name": "dependency_parser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
