{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dwdb/dependency-parser/blob/master/fast_dpnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "jYT5zZuMV9O8",
    "outputId": "b9264177-df1b-414a-82fb-da121e4cd71a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # 只设置 GPU 5 为可见设备（忽略其他 GPU）\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        \n",
    "        # 如果需要按需分配内存（可选）\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def print_device_info():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        print(f\"Device: {device.name}, Type: {device.device_type}\")\n",
    "\n",
    "print_device_info()  # 在程序开始时调用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaJUfn_MWHuq"
   },
   "source": [
    "# 依存句法单词节点类\n",
    "节点id、词性、头节点id、左右孩子节点（指向的节点）、左右依存关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC0n1fvYV-uf"
   },
   "outputs": [],
   "source": [
    "class Token(object):\n",
    "\n",
    "    def __init__(self, token_id, word, pos, dep, head_id):\n",
    "        self.token_id = token_id\n",
    "        self.word = word.lower()\n",
    "        self.pos = pos\n",
    "        if head_id >= token_id:\n",
    "            self.dep = 'L_' + dep\n",
    "        else:\n",
    "            self.dep = 'R_' + dep\n",
    "        self.head_id = head_id\n",
    "        self.left = []\n",
    "        self.right = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Token(token_id={token_id},word={word},head_id={head_id})'.format(\n",
    "            **self.__dict__)\n",
    "\n",
    "\n",
    "ROOT_TOKEN = Token(-1, '<root>', '<root>', '<root>', -1)\n",
    "NULL_TOKEN = Token(-1, '<null>', '<null>', '<null>', -1)\n",
    "UNK_TOKEN = Token(-1, '<unk>', '<unk>', '<unk>', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxK-Z8YGWtff"
   },
   "source": [
    "# 依存句法transfer-reduce句子类\n",
    "节点、缓冲、栈，通过get_next_input可获取当前状态下的单词、磁性、依存关系共计18+18+12个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9Tbr-9PWGZe"
   },
   "outputs": [],
   "source": [
    "class Sentence(object):\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.buff = tokens.copy()\n",
    "        self.stack = [ROOT_TOKEN]\n",
    "        self.deps = []\n",
    "\n",
    "    def update_by_action(self, action=None):\n",
    "        if action is None:\n",
    "            action = self.get_action()\n",
    "        # 转移\n",
    "        if action == 'shift':\n",
    "            self.stack.append(self.buff.pop(0))\n",
    "        # 左弧 stack1 <- stack0\n",
    "        elif action.startswith('L_'):\n",
    "            token = self.stack.pop(-2)\n",
    "            token.dep = action\n",
    "            self.deps.append((self.stack[-1].token_id, token.token_id, action))\n",
    "            self.binary_insert(self.stack[-1].left, token)\n",
    "        # 右弧 stack1 -> stack0\n",
    "        elif action.startswith('R_'):\n",
    "            token = self.stack.pop(-1)\n",
    "            token.dep = action\n",
    "            self.deps.append((self.stack[-1].token_id, token.token_id, action))\n",
    "            self.binary_insert(self.stack[-1].right, token)\n",
    "        else:\n",
    "            raise ValueError('unknown state!')\n",
    "        return action\n",
    "\n",
    "    def get_action(self):\n",
    "        if len(self.stack) < 2:\n",
    "            return 'shift'\n",
    "        t1, t0 = self.stack[-2:]\n",
    "        # left arc\n",
    "        if t1.head_id == t0.token_id:\n",
    "            return t1.dep\n",
    "        # right arc\n",
    "        if t0.head_id == t1.token_id:\n",
    "            if any(t0.token_id == t.head_id for t in self.buff):\n",
    "                return 'shift'\n",
    "            return t0.dep\n",
    "        return 'shift'\n",
    "\n",
    "    def get_next_input(self, word2id, pos2id, dep2id):\n",
    "        \"\"\"将conll dataset转为fast dependency parser dataset\n",
    "        18 features of words and pos tags:\n",
    "            The top 3 words on the stack and buffer:\n",
    "                s1, s2, s3, b1, b2, b3\n",
    "            The first and second leftmost/rightmost children of top 2 words on stack:\n",
    "                lc1(s1), rc1(s1), lc2(s1), rc2(s1), lc1(s2), rc1(s2), lc2(s2), rc2(s2)\n",
    "            The leftmost/rightmost of leftmost/rightmost of top 2 words on stack:\n",
    "                lc1(lc1(s1)), rc1(rc1(s1)), lc1(lc1(s2)), rc1(rc1(s2))\n",
    "        12 features of dependencies, that excluding those 6 (18-6=12) words on the stack/buffer\n",
    "        \"\"\"\n",
    "\n",
    "        def pad_tokens(tokens, maxlen):\n",
    "            tokens = tokens[:maxlen]\n",
    "            if len(tokens) < maxlen:\n",
    "                tokens += [NULL_TOKEN] * (maxlen - len(tokens))\n",
    "            return tokens\n",
    "\n",
    "        def get_children(token):\n",
    "            lc1, lc2 = pad_tokens(token.left, 2)\n",
    "            rc1, rc2 = pad_tokens(token.right, 2)\n",
    "            llc1, = pad_tokens(lc1.left, 1)\n",
    "            rrc1, = pad_tokens(rc1.right, 1)\n",
    "            return [lc1, rc1, lc2, rc2, llc1, rrc1]\n",
    "\n",
    "        # top 3 features on stack\n",
    "        s1, s2, s3 = pad_tokens(self.stack[-1::-1], 3)\n",
    "        # 18 features\n",
    "        tokens = [s1, s2, s3] + pad_tokens(self.buff, 3) + get_children(s1) + get_children(s2)\n",
    "        # word, pos tag and dependency indices\n",
    "        input_word = [word2id.get(token.word, word2id[UNK_TOKEN.word]) for token in tokens]\n",
    "        input_pos = [pos2id.get(token.pos, pos2id[UNK_TOKEN.pos]) for token in tokens]\n",
    "        input_dep = [dep2id.get(token.dep, dep2id[UNK_TOKEN.dep]) for token in tokens[6:]]\n",
    "        return input_word, input_pos, input_dep\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_insert(array, value, key=lambda x: x.token_id):\n",
    "        start, end = 0, len(array) - 1\n",
    "        while start <= end:\n",
    "            mid = int((start + end) / 2)\n",
    "            if key(value) >= key(array[mid]):\n",
    "                start = mid + 1\n",
    "            else:\n",
    "                end = mid - 1\n",
    "        array.insert(start, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axuRWJxnXSHS"
   },
   "source": [
    "# Conll数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SomeK2tBXScv",
    "outputId": "76dfca31-4dbf-4f81-f923-4c05b3189b7f"
   },
   "outputs": [],
   "source": [
    "class ConllDataset(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, encoding='utf8') as f:\n",
    "            dataset, tokens = [], []\n",
    "            for line in f.readlines():\n",
    "                if line == '\\n':\n",
    "                    dataset.append(Sentence(tokens))\n",
    "                    tokens = []\n",
    "                else:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    token = Token(int(line[0]) - 1, line[1], line[4], line[7], int(line[6]) - 1)\n",
    "                    tokens.append(token)\n",
    "            if tokens:\n",
    "                dataset.append(Sentence(tokens))\n",
    "        return dataset\n",
    "\n",
    "    def fit_transform(self, path, min_count=2, shuffle=True):\n",
    "        dataset = self.load(path)\n",
    "        # build vocabulary\n",
    "        vocab = defaultdict(int)\n",
    "        pos_tags, deps = set(), set()\n",
    "        for sentence in dataset:\n",
    "            for token in sentence.tokens:\n",
    "                vocab[token.word] += 1\n",
    "                pos_tags.add(token.pos)\n",
    "                deps.add(token.dep)\n",
    "\n",
    "        # create word dictionary\n",
    "        vocab = {k for k, v in vocab.items() if v >= min_count}\n",
    "        vocab.update((ROOT_TOKEN.word, NULL_TOKEN.word, UNK_TOKEN.word))\n",
    "        self.word2id = dict(zip(sorted(vocab), range(len(vocab))))\n",
    "        # create part-of-speech dictionary\n",
    "        pos_tags.update((ROOT_TOKEN.pos, NULL_TOKEN.pos, UNK_TOKEN.pos))\n",
    "        self.pos2id = dict(zip(sorted(pos_tags), range(len(pos_tags))))\n",
    "        # create dependency and labelsdictionary\n",
    "        deps.update((ROOT_TOKEN.dep, NULL_TOKEN.dep, UNK_TOKEN.dep))\n",
    "        labels = deps.copy() | {'shift', UNK_TOKEN.dep}\n",
    "        self.dep2id = dict(zip(sorted(deps), range(len(deps))))\n",
    "        self.id2label = sorted(labels)\n",
    "        self.label2id = dict(zip(self.id2label, range(len(self.id2label))))\n",
    "\n",
    "        self._fit = True\n",
    "        dataset = self.transform(path, dataset=dataset, shuffle=shuffle)\n",
    "        return dataset\n",
    "\n",
    "    def transform(self, path=None, dataset=None, shuffle=False):\n",
    "        if not dataset and path:\n",
    "            dataset = self.load(path)\n",
    "        assert getattr(self, '_fit', None), 'Model must be fit before transform!'\n",
    "\n",
    "        error_count = 0\n",
    "        inputs = []\n",
    "        for i, sentence in enumerate(dataset):\n",
    "            if i % 5000 == 0:\n",
    "                print('transforming at line %d' % i)\n",
    "            while len(sentence.stack) > 1 or sentence.buff:\n",
    "                input_word, input_pos, input_dep = sentence.get_next_input(\n",
    "                    self.word2id, self.pos2id, self.dep2id)\n",
    "                try:\n",
    "                    output = sentence.update_by_action()\n",
    "                    output = self.label2id.get(output, self.label2id[UNK_TOKEN.dep])\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    error_count += 1\n",
    "                    break\n",
    "                inputs.append((input_word, input_pos, input_dep, output))\n",
    "\n",
    "        # shuffle dataset\n",
    "        if shuffle:\n",
    "            np.random.shuffle(inputs)\n",
    "\n",
    "        print('%s: error count:%d, total count:%d, total examples:%d' % (\n",
    "            os.path.basename(str(path)), error_count, len(dataset), len(inputs)))\n",
    "        # input_word, input_pos, input_dep, output\n",
    "        return tuple(np.array(data, np.int32) for data in zip(*inputs))\n",
    "\n",
    "\n",
    "conll_path = '/data/fyj/project/dependency-parser/data/conll'\n",
    "conll = ConllDataset()\n",
    "train_dataset = conll.fit_transform(os.path.join(conll_path, 'train.conll'))\n",
    "valid_dataset = conll.transform(os.path.join(conll_path, 'dev.conll'))\n",
    "\n",
    "output_path = './output/'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "pickle.dump(conll.word2id, open(os.path.join(output_path, 'word2id.pkl'), 'wb'))\n",
    "pickle.dump(conll.pos2id, open(os.path.join(output_path, 'pos2id.pkl'), 'wb'))\n",
    "pickle.dump(conll.dep2id, open(os.path.join(output_path, 'dep2id.pkl'), 'wb'))\n",
    "pickle.dump(conll.label2id, open(os.path.join(output_path, 'label2id.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpTRzReiY9zs"
   },
   "source": [
    "# 创建fast dependency model\n",
    "单词、词性、依存关系存于同一字典，只需创建一个embedding变量即可，但是这样做不好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "NR-GAo9tYnhF",
    "outputId": "c7c3f03b-b23c-4ae6-e593-4d4bccaa0dce"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, pos_size, dep_size, embedding_size, n_classes):\n",
    "    print('vocab_size:%d, pos_size:%d, dep_size:%d, n_classes:%d' % (\n",
    "        vocab_size, pos_size, dep_size, n_classes))\n",
    "    l2_regularizer = tf.keras.regularizers.l2(1e-5)\n",
    "    # input layer\n",
    "    input_word = tf.keras.layers.Input(shape=(18,))\n",
    "    input_pos = tf.keras.layers.Input(shape=(18,))\n",
    "    input_dep = tf.keras.layers.Input(shape=(12,))\n",
    "    # embedding layer，initial weight range of [-0.01, 0.01]\n",
    "    word_embedding = tf.keras.layers.Embedding(\n",
    "        vocab_size, embedding_size,\n",
    "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
    "        embeddings_regularizer=l2_regularizer)(input_word)\n",
    "    pos_embedding = tf.keras.layers.Embedding(\n",
    "        pos_size, embedding_size,\n",
    "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
    "        embeddings_regularizer=l2_regularizer)(input_pos)\n",
    "    dep_embedding = tf.keras.layers.Embedding(\n",
    "        dep_size, embedding_size,\n",
    "        embeddings_initializer=tf.keras.initializers.RandomUniform(-0.01, 0.01),\n",
    "        embeddings_regularizer=l2_regularizer)(input_dep)\n",
    "    # shape=(batch_size, 48, embedding_size)\n",
    "    embedding = tf.concat((word_embedding, pos_embedding, dep_embedding), axis=1)\n",
    "    embedding = tf.reshape(embedding, shape=(-1, embedding_size * 48))\n",
    "    embedding = tf.keras.layers.Dropout(rate=0.4)(embedding)\n",
    "    # dense layer\n",
    "    dense1 = tf.keras.layers.Dense(\n",
    "        units=100,\n",
    "        kernel_regularizer=l2_regularizer,\n",
    "        bias_regularizer=l2_regularizer)(embedding)\n",
    "    dense1 = tf.pow(dense1, 3)\n",
    "    dense1 = tf.keras.layers.Dropout(rate=0.4)(dense1)\n",
    "    # dense layer\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=n_classes,\n",
    "        kernel_regularizer=l2_regularizer,\n",
    "        bias_regularizer=l2_regularizer)(dense1)\n",
    "    model = tf.keras.Model(inputs=(input_word, input_pos, input_dep), outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocab_size=len(conll.word2id), pos_size=len(conll.pos2id), \n",
    "                    dep_size=len(conll.dep2id), embedding_size=50, n_classes=len(conll.label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "G0SXPJgRtaxZ",
    "outputId": "1ca279cd-7780-49a2-b9de-abdf2ff050c8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset[:3], train_dataset[3],\n",
    "    batch_size=2048, epochs=10, validation_data=(valid_dataset[:3], valid_dataset[3]))\n",
    "model.save(os.path.join(output_path, 'checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I95_moDHmggh",
    "outputId": "bd885c9a-6a99-4f64-fe2d-d7ef5c38e09e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# 获取当前可用设备\n",
    "\n",
    "\n",
    "# 继续执行程序\n",
    "\n",
    "final_res=[]\n",
    "def evaluate(sentences, output_path):\n",
    "    print_device_info()\n",
    "    # restore model\n",
    "    model = tf.keras.models.load_model(os.path.join(output_path, 'checkpoint'))\n",
    "    # load local dictionary\n",
    "    label2id = pickle.load(open(os.path.join(output_path, 'label2id.pkl'), 'rb'))\n",
    "    word2id = pickle.load(open(os.path.join(output_path, 'word2id.pkl'), 'rb'))\n",
    "    pos2id = pickle.load(open(os.path.join(output_path, 'pos2id.pkl'), 'rb'))\n",
    "    dep2id = pickle.load(open(os.path.join(output_path, 'dep2id.pkl'), 'rb'))\n",
    "    id2label, _ = zip(*sorted(label2id.items(), key=lambda x:x[1]))\n",
    "    \n",
    "    uas = las = count = 0\n",
    "    time_record = time.time()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        raw_deps = {}\n",
    "        for token in sentence.tokens:\n",
    "            raw_deps[(token.head_id, token.token_id)] = token.dep\n",
    "            token.dep = NULL_TOKEN.dep\n",
    "            token.head_id = -1\n",
    "        count += len(sentence.tokens)\n",
    "        while len(sentence.stack) > 1 or sentence.buff:\n",
    "            input_word, input_pos, input_dep = sentence.get_next_input(word2id, pos2id, dep2id)\n",
    "            input_word = np.array([input_word], dtype=np.int32)\n",
    "            input_pos = np.array([input_pos], dtype=np.int32)\n",
    "            input_dep = np.array([input_dep], dtype=np.int32)\n",
    "            output = model.predict((input_word, input_pos, input_dep))\n",
    "            action = id2label[np.argmax(output[0])]\n",
    "            try:\n",
    "                sentence.update_by_action(action)\n",
    "            except (IndexError, ValueError):\n",
    "                # print(len(sentence.tokens), len(sentence.deps))\n",
    "                break\n",
    "\n",
    "        for head, tail, action in sentence.deps:\n",
    "            raw_action = raw_deps.get((head, tail))\n",
    "            if raw_action is not None:\n",
    "                uas += 1\n",
    "                if raw_action == action:\n",
    "                    las += 1\n",
    "        if (i + 1) % 10 == 0 or i==len(sentences):\n",
    "            print('total sentence:%d, UAS:%.3f, LAS:%.3f' % (i + 1, uas / count, las / count))\n",
    "            res={}\n",
    "            res['uas']=uas / count\n",
    "            res['las']=las / count\n",
    "            res['second']=time.time()-time_record\n",
    "            final_res.append(res)\n",
    "\n",
    "sentences = ConllDataset.load(os.path.join(conll_path, 'dev.conll'))\n",
    "evaluate(sentences, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPc7xPWbKe6JMtLlTIDT0Wd",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1e_GKf3_9rUaRtgSPBknQYQ57h-W2dWZM",
   "name": "fast_dpnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "fyj_dp",
   "language": "python",
   "name": "dependency_parser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
